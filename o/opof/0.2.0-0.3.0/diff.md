# Comparing `tmp/opof-0.2.0-py3-none-any.whl.zip` & `tmp/opof-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,36 @@
-Zip file size: 32566 bytes, number of entries: 33
--rw-r--r--  2.0 unx      369 b- defN 23-Mar-12 17:56 opof/__init__.py
--rw-r--r--  2.0 unx     1264 b- defN 23-Mar-12 17:56 opof/algorithm.py
--rw-r--r--  2.0 unx     2379 b- defN 23-Mar-12 17:56 opof/domain.py
--rw-r--r--  2.0 unx      829 b- defN 23-Mar-12 17:56 opof/evaluator.py
--rw-r--r--  2.0 unx     1022 b- defN 23-Mar-12 17:56 opof/generator.py
--rw-r--r--  2.0 unx     2315 b- defN 23-Mar-12 17:56 opof/parameter_space.py
--rw-r--r--  2.0 unx     1098 b- defN 23-Mar-12 17:56 opof/planner.py
--rw-r--r--  2.0 unx      870 b- defN 23-Mar-12 17:56 opof/problem_set.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-12 17:56 opof/py.typed
--rw-r--r--  2.0 unx       68 b- defN 23-Mar-12 17:56 opof/algorithms/__init__.py
--rw-r--r--  2.0 unx    15131 b- defN 23-Mar-12 17:56 opof/algorithms/gc.py
--rw-r--r--  2.0 unx     7576 b- defN 23-Mar-12 17:56 opof/algorithms/smac.py
--rw-r--r--  2.0 unx       71 b- defN 23-Mar-12 17:56 opof/evaluators/__init__.py
--rw-r--r--  2.0 unx     3613 b- defN 23-Mar-12 17:56 opof/evaluators/list_evaluator.py
--rw-r--r--  2.0 unx      163 b- defN 23-Mar-12 17:56 opof/models/__init__.py
--rw-r--r--  2.0 unx     7430 b- defN 23-Mar-12 17:56 opof/models/fc_resnet.py
--rw-r--r--  2.0 unx     1130 b- defN 23-Mar-12 17:56 opof/models/static.py
--rw-r--r--  2.0 unx      147 b- defN 23-Mar-12 17:56 opof/parameter_spaces/__init__.py
--rw-r--r--  2.0 unx     4578 b- defN 23-Mar-12 17:56 opof/parameter_spaces/interval.py
--rw-r--r--  2.0 unx     7885 b- defN 23-Mar-12 17:56 opof/parameter_spaces/power_spherical.py
--rw-r--r--  2.0 unx     5412 b- defN 23-Mar-12 17:56 opof/parameter_spaces/simplex.py
--rw-r--r--  2.0 unx     6200 b- defN 23-Mar-12 17:56 opof/parameter_spaces/sphere.py
--rw-r--r--  2.0 unx       65 b- defN 23-Mar-12 17:56 opof/problem_sets/__init__.py
--rw-r--r--  2.0 unx     1495 b- defN 23-Mar-12 17:56 opof/problem_sets/problem_list.py
--rw-r--r--  2.0 unx      388 b- defN 23-Mar-12 17:56 opof/registry/__init__.py
--rw-r--r--  2.0 unx     4226 b- defN 23-Mar-12 17:56 opof/registry/utils.py
--rwxr-xr-x  2.0 unx      910 b- defN 23-Mar-12 17:56 opof-0.2.0.data/scripts/opof-registry
--rwxr-xr-x  2.0 unx     1711 b- defN 23-Mar-12 17:56 opof-0.2.0.data/scripts/opof-train
--rw-r--r--  2.0 unx     1461 b- defN 23-Mar-12 17:56 opof-0.2.0.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     5350 b- defN 23-Mar-12 17:56 opof-0.2.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-12 17:56 opof-0.2.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 23-Mar-12 17:56 opof-0.2.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2653 b- defN 23-Mar-12 17:56 opof-0.2.0.dist-info/RECORD
-33 files, 87906 bytes uncompressed, 28344 bytes compressed:  67.8%
+Zip file size: 35501 bytes, number of entries: 34
+-rw-r--r--  2.0 unx      369 b- defN 23-Apr-30 16:02 opof/__init__.py
+-rw-r--r--  2.0 unx     1264 b- defN 23-Apr-30 16:02 opof/algorithm.py
+-rw-r--r--  2.0 unx     2379 b- defN 23-Apr-30 16:02 opof/domain.py
+-rw-r--r--  2.0 unx      829 b- defN 23-Apr-30 16:02 opof/evaluator.py
+-rw-r--r--  2.0 unx      994 b- defN 23-Apr-30 16:02 opof/generator.py
+-rw-r--r--  2.0 unx     2315 b- defN 23-Apr-30 16:02 opof/parameter_space.py
+-rw-r--r--  2.0 unx     1094 b- defN 23-Apr-30 16:02 opof/planner.py
+-rw-r--r--  2.0 unx      870 b- defN 23-Apr-30 16:02 opof/problem_set.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-30 16:02 opof/py.typed
+-rw-r--r--  2.0 unx      102 b- defN 23-Apr-30 16:02 opof/algorithms/__init__.py
+-rw-r--r--  2.0 unx    16384 b- defN 23-Apr-30 16:02 opof/algorithms/gc.py
+-rw-r--r--  2.0 unx     7069 b- defN 23-Apr-30 16:02 opof/algorithms/pypop.py
+-rw-r--r--  2.0 unx     8007 b- defN 23-Apr-30 16:02 opof/algorithms/smac.py
+-rw-r--r--  2.0 unx       71 b- defN 23-Apr-30 16:02 opof/evaluators/__init__.py
+-rw-r--r--  2.0 unx     3629 b- defN 23-Apr-30 16:02 opof/evaluators/list_evaluator.py
+-rw-r--r--  2.0 unx      163 b- defN 23-Apr-30 16:02 opof/models/__init__.py
+-rw-r--r--  2.0 unx     7475 b- defN 23-Apr-30 16:02 opof/models/fc_resnet.py
+-rw-r--r--  2.0 unx     1295 b- defN 23-Apr-30 16:02 opof/models/static.py
+-rw-r--r--  2.0 unx      147 b- defN 23-Apr-30 16:02 opof/parameter_spaces/__init__.py
+-rw-r--r--  2.0 unx     4578 b- defN 23-Apr-30 16:02 opof/parameter_spaces/interval.py
+-rw-r--r--  2.0 unx     7885 b- defN 23-Apr-30 16:02 opof/parameter_spaces/power_spherical.py
+-rw-r--r--  2.0 unx     5412 b- defN 23-Apr-30 16:02 opof/parameter_spaces/simplex.py
+-rw-r--r--  2.0 unx     6236 b- defN 23-Apr-30 16:02 opof/parameter_spaces/sphere.py
+-rw-r--r--  2.0 unx       65 b- defN 23-Apr-30 16:02 opof/problem_sets/__init__.py
+-rw-r--r--  2.0 unx     1495 b- defN 23-Apr-30 16:02 opof/problem_sets/problem_list.py
+-rw-r--r--  2.0 unx      388 b- defN 23-Apr-30 16:02 opof/registry/__init__.py
+-rw-r--r--  2.0 unx     4226 b- defN 23-Apr-30 16:02 opof/registry/utils.py
+-rwxr-xr-x  2.0 unx      910 b- defN 23-Apr-30 16:02 opof-0.3.0.data/scripts/opof-registry
+-rwxr-xr-x  2.0 unx     2469 b- defN 23-Apr-30 16:02 opof-0.3.0.data/scripts/opof-train
+-rw-r--r--  2.0 unx     1461 b- defN 23-Apr-30 16:02 opof-0.3.0.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx     4908 b- defN 23-Apr-30 16:02 opof-0.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-30 16:02 opof-0.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 23-Apr-30 16:02 opof-0.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2734 b- defN 23-Apr-30 16:02 opof-0.3.0.dist-info/RECORD
+34 files, 97320 bytes uncompressed, 31155 bytes compressed:  68.0%
```

## zipnote {}

```diff
@@ -27,14 +27,17 @@
 
 Filename: opof/algorithms/__init__.py
 Comment: 
 
 Filename: opof/algorithms/gc.py
 Comment: 
 
+Filename: opof/algorithms/pypop.py
+Comment: 
+
 Filename: opof/algorithms/smac.py
 Comment: 
 
 Filename: opof/evaluators/__init__.py
 Comment: 
 
 Filename: opof/evaluators/list_evaluator.py
@@ -72,29 +75,29 @@
 
 Filename: opof/registry/__init__.py
 Comment: 
 
 Filename: opof/registry/utils.py
 Comment: 
 
-Filename: opof-0.2.0.data/scripts/opof-registry
+Filename: opof-0.3.0.data/scripts/opof-registry
 Comment: 
 
-Filename: opof-0.2.0.data/scripts/opof-train
+Filename: opof-0.3.0.data/scripts/opof-train
 Comment: 
 
-Filename: opof-0.2.0.dist-info/LICENSE.md
+Filename: opof-0.3.0.dist-info/LICENSE.md
 Comment: 
 
-Filename: opof-0.2.0.dist-info/METADATA
+Filename: opof-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: opof-0.2.0.dist-info/WHEEL
+Filename: opof-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: opof-0.2.0.dist-info/top_level.txt
+Filename: opof-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: opof-0.2.0.dist-info/RECORD
+Filename: opof-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## opof/generator.py

```diff
@@ -1,24 +1,27 @@
 from abc import ABCMeta, abstractmethod
-from typing import Generic, List, Optional, Tuple, TypeVar, Any
+from typing import Any, Generic, List, Optional, Tuple, TypeVar
 
 from torch import Tensor, nn
 
 Problem = TypeVar("Problem")
 
 
 class Generator(Generic[Problem], nn.Module, metaclass=ABCMeta):
     """
     :class:`Generator` is the abstract base class representing a mapping :math:`G_\\theta`
-    from problem instances :math:`c \\in \\mathcal{C}` to distributions over the planning 
+    from problem instances :math:`c \\in \\mathcal{C}` to distributions over the planning
     parameter space :math:`\\mathcal{X}`.
     """
+
     @abstractmethod
-    def forward(self, problem: List[Problem]) -> Tuple[List[Tensor], Optional[Tensor], List[Any]]:
+    def forward(
+        self, problem: List[Problem]
+    ) -> Tuple[List[Tensor], Optional[Tensor], List[List[Any]]]:
         """
         Transforms a batch of problem instances :math:`c \\in \\mathcal{C}` into a batch
         of distributions over :math:`\\mathcal{X}`, and returns a batch of samples,
-        an optional batch the entropies, and extra objects which are passed to the planner.
+        entropies, and extra objects which are passed to the planner.
 
         :param problem: Batch of problem instances
-        :return: Batch of samples, optional batch of entropies, and list of extra objects.
+        :return: Batch of samples, entropies, and extra objects.
         """
```

## opof/planner.py

```diff
@@ -19,10 +19,10 @@
         """
         Performs some computation for the given problem instance, planning parameters, 
         and extra objects, and returns a sample of the planning objective, along with 
         other optional metrics and objects.
 
         :param problem: Problem instance
         :param parameters: Planning parameters
-        :param parameters: Optional objects
+        :param extras: Optional objects
         :return: Dictionary consisting planning objective and other optional metrics and objects. The ``\"objective\"`` key **must** be present as a :class:`float` value.
         """
```

## opof/algorithms/__init__.py

```diff
@@ -1,4 +1,5 @@
 from .gc import GC
 from .smac import SMAC
+from .pypop import PyPop
 
-__all__ = ["GC", "SMAC"]
+__all__ = ["GC", "SMAC", "PyPop"]
```

## opof/algorithms/gc.py

```diff
@@ -1,17 +1,18 @@
 import itertools
 import os
+import shutil
 from multiprocessing import Process, Queue
 from typing import Any, List, Optional, TypeVar
 
 import numpy as np
 import torch
 import torch.nn.utils
 from torch.optim import Adam
-from torch.utils.tensorboard.writer import SummaryWriter
+from torch.utils.tensorboard import SummaryWriter
 
 from ..algorithm import Algorithm
 from ..domain import Domain
 from ..evaluator import Evaluator
 from ..models import FCResNetCritic, FCResNetGenerator
 from ..registry import concurrency
 
@@ -52,33 +53,32 @@
 
 Problem = TypeVar("Problem")
 
 
 class GC(Algorithm):
     """
     :class:`GC` is our implementation of the Generator-Critic algorithm introduced across the recent works of [Lee2021a]_, [Lee2022a]_, and [Danesh2022a]_.
-
     Two neural networks are learned simultaneously -- a *generator network* representing the generator :math:`G_\\theta(c)` and a *critic network*.
     The generator is stochastic, and maps a problem instance :math:`c \\in \\mathcal{C}` to a sample of planning parameters :math:`x \\in \\mathcal{X}`.
     The critic maps :math:`c` and :math:`x` into a real distribution modeling :math:`\\boldsymbol{f}(x; c)`.
-
     During training, the stochastic generator takes a random problem instance :math:`c \\in \\mathcal{C}` and produces a sample :math:`x \\in \\mathcal{X}`,
     which is used to probe the planner. The planner's response, along with :math:`c` and :math:`x`, are used to update the critic via supervision loss.
     The critic then acts as a *differentiable surrogate objective* for :math:`\\boldsymbol{f}(x; c)`, passing gradients to the generator via the chain rule.
     """
 
     device: torch.device
     dtype: torch.dtype
 
     num_workers: int
     lr: float
     alpha_lr: float
     max_buffer_size: int
     min_buffer_size: int
     batch_size: int
+    requires_entropy: Optional[bool]
 
     iterations: int
 
     eval_interval: int
     evaluator: Evaluator
     save_interval: int
 
@@ -127,14 +127,15 @@
         self.lr = lr
         self.alpha_lr = alpha_lr if alpha_lr is not None else 3 * self.lr
         self.min_buffer_size = min_buffer_size
         self.max_buffer_size = (
             max_buffer_size if max_buffer_size is not None else self.iterations
         )
         self.batch_size = batch_size
+        self.requires_entropy = None
 
         self.eval_interval = eval_interval
         self.evaluator = self.domain.create_evaluator()
         self.save_interval = (
             save_interval if save_interval is not None else int(self.iterations / 100)
         )
 
@@ -156,14 +157,16 @@
             worker.start()
         for worker in workers:
             init_queue.get()
 
         # Logger.
         logger: Optional[SummaryWriter] = None
         if self.eval_folder is not None:
+            if os.path.exists(self.eval_folder):
+                shutil.rmtree(self.eval_folder)
             logger = SummaryWriter(self.eval_folder)
 
         try:
             # Replay buffer.
             buffer = []
 
             # Models.
@@ -238,70 +241,90 @@
                             if p is not None:
                                 torch.nan_to_num(p)
 
                     # Update generator.
                     critic.eval()
                     generator.train()
                     (parameters, entropy, _) = generator(problem)
+
                     # Infer log_alpha dimensions lazily from entropy.
-                    if log_alpha is None:
-                        log_alpha = torch.tensor(
-                            np.array(
-                                [LOG_ALPHA_INIT] * entropy[0].reshape(-1).shape[0]
-                            ),
-                            requires_grad=True,
-                            device=self.device,
-                            dtype=self.dtype,
-                        )
-                        entropy_target = torch.tensor(
-                            list(
-                                itertools.chain(
-                                    *[
-                                        space.dist_target_entropy
-                                        for space in self.domain.composite_parameter_space()
-                                    ]
-                                )
-                            ),
-                            requires_grad=False,
-                            device=self.device,
-                            dtype=self.dtype,
-                        )
-                        alpha_optim = torch.optim.Adam([log_alpha], lr=self.alpha_lr)
-                    entropy = entropy.reshape(-1, log_alpha.shape[0])
+                    if self.requires_entropy == None:
+                        self.requires_entropy = entropy is not None
+                        if self.requires_entropy:
+                            log_alpha = torch.tensor(
+                                np.array(
+                                    [LOG_ALPHA_INIT] * entropy[0].reshape(-1).shape[0]
+                                ),
+                                requires_grad=True,
+                                device=self.device,
+                                dtype=self.dtype,
+                            )
+                            entropy_target = torch.tensor(
+                                list(
+                                    itertools.chain(
+                                        *[
+                                            space.dist_target_entropy
+                                            for space in self.domain.composite_parameter_space()
+                                        ]
+                                    )
+                                ),
+                                requires_grad=False,
+                                device=self.device,
+                                dtype=self.dtype,
+                            )
+                            alpha_optim = torch.optim.Adam(
+                                [log_alpha], lr=self.alpha_lr
+                            )
+
+                    # Compute generator term.
                     generator_performance = critic(problem, parameters).mean
-                    dual = (log_alpha.exp().detach() * entropy).sum(dim=-1)
                     debug_generator_perf = generator_performance.mean().detach().item()
-                    debug_entropy = entropy.mean().detach().item()
-                    if is_valid(generator_performance) and is_valid(dual):
+
+                    # Compute entropy terms.
+                    if self.requires_entropy == True:
+                        assert log_alpha is not None
+                        entropy = entropy.reshape(-1, log_alpha.shape[0])
+                        dual = (log_alpha.exp().detach() * entropy).sum(dim=-1)
+                        debug_entropy = entropy.mean().detach().item()
+
+                    if is_valid(generator_performance) and (
+                        self.requires_entropy == False or is_valid(dual)
+                    ):
                         generator_optim.zero_grad()
                         # We want to maximize performance and dual, but torch does
                         # minimization. So we use the negative.
-                        (-generator_performance - dual).mean().backward()
+                        if self.requires_entropy == True:
+                            (-generator_performance - dual).mean().backward()
+                        else:
+                            (-generator_performance).mean().backward()
                         torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)
                         for p in generator.parameters():
                             if p is not None and p.grad is not None:
                                 torch.nan_to_num(p.grad)
                         generator_optim.step()
                         for p in generator.parameters():
                             if p is not None:
                                 torch.nan_to_num(p)
 
-                    # Update entropies.
-                    # Update log alpha.
-                    alpha_optim.zero_grad()
-                    alpha_loss = log_alpha * ((entropy - entropy_target).detach())
-                    alpha_loss.mean().backward()
-                    assert log_alpha.grad is not None
-                    with torch.no_grad():
-                        log_alpha.grad *= (
-                            ((-log_alpha.grad >= 0) | (log_alpha >= LOG_ALPHA_MIN))
-                            & ((-log_alpha.grad < 0) | (log_alpha <= LOG_ALPHA_MAX))
-                        ).type(self.dtype)
-                    alpha_optim.step()
-                    debug_log_alpha = log_alpha.mean().item()
+                    if self.requires_entropy == True:
+                        assert alpha_optim is not None
+                        assert log_alpha is not None
+                        # Update entropies.
+                        # Update log alpha.
+                        alpha_optim.zero_grad()
+                        alpha_loss = log_alpha * ((entropy - entropy_target).detach())
+                        alpha_loss.mean().backward()
+                        assert log_alpha.grad is not None
+                        with torch.no_grad():
+                            log_alpha.grad *= (
+                                ((-log_alpha.grad >= 0) | (log_alpha >= LOG_ALPHA_MIN))
+                                & ((-log_alpha.grad < 0) | (log_alpha <= LOG_ALPHA_MAX))
+                            ).type(self.dtype)
+                        alpha_optim.step()
+                        debug_log_alpha = log_alpha.mean().item()
 
                     debug_recent = sum(r[2] for r in buffer[-100:]) / 100
 
                     # Evaluate.
                     it = iteration if iteration == 0 else iteration + 1
                     if it % self.eval_interval == 0:
                         generator.eval()
@@ -329,16 +352,17 @@
 
                     # Logging.
                     iteration += 1
                     debug: List[Any] = []
                     debug.append(iteration)
                     debug.append(debug_critic_accuracy)
                     debug.append(debug_generator_perf)
-                    debug.append(debug_entropy)
-                    debug.append(debug_log_alpha)
+                    if self.requires_entropy == True:
+                        debug.append(debug_entropy)
+                        debug.append(debug_log_alpha)
                     debug.append(debug_recent)
                     if (
                         self.eval_interval is None
                         or iteration % max(1, int(self.eval_interval / 100)) == 0
                     ):
                         print("\033[K", end="")
                         print(
@@ -349,15 +373,16 @@
                 # Respond to pending requests only after results have cleared.
                 problem = []
                 while request_queue.qsize() > 0:
                     # Get request.
                     problem.append(request_queue.get())
                 if len(problem) > 0:
                     generator.eval()
-                    (parameters, _, extras) = generator(problem)
+                    with torch.no_grad():
+                        (parameters, _, extras) = generator(problem)
                     # Submit jobs.
                     for (i, _problem) in enumerate(problem):
                         _parameters = [p[i].detach().cpu().numpy() for p in parameters]
-                        job_queue.put((_problem, _parameters, extras))
+                        job_queue.put((_problem, _parameters, [e[i] for e in extras]))
         finally:
             for _ in workers:
                 job_queue.put(None)
```

## opof/algorithms/smac.py

```diff
@@ -1,17 +1,20 @@
+import os
+import shutil
 from multiprocessing import Process, Queue
 from pathlib import Path
 from typing import Optional
 
 import torch
 from ConfigSpace import Configuration, ConfigurationSpace, Float
 from smac import Callback, HyperparameterOptimizationFacade, Scenario
-from torch.utils.tensorboard.writer import SummaryWriter
+from torch.utils.tensorboard import SummaryWriter
+
+from opof.algorithm import Algorithm
 
-from ..algorithm import Algorithm
 from ..domain import Domain
 from ..models import StaticGenerator
 from ..registry import concurrency
 
 
 def smac_worker(domain: Domain, job_queue: Queue, result_queue: Queue):
     # Create planner.
@@ -24,29 +27,28 @@
             break
         result = planner(job[0], job[1], job[2])
         result_queue.put((job[0], job[1], result))
 
 
 class SMAC(Algorithm):
     """
-    :class:`SMAC` is a wrapper around the `SMAC3 package <https://github.com/automl/SMAC3>`_, 
-    an actively maintained tool for algorithm configuration using the latest Bayesian optimization techniques. 
-    We use the `HPOFacade <https://automl.github.io/SMAC3/main/api/smac.facade.hyperparameter_optimization_facade.html#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade>`_ 
-    strategy provided by SMAC3. 
-
-    In the context of the planner optimization problem, SMAC learns only a generator :math:`G_\\theta(c) = \\theta` that is unconditional 
-    (i.e. does not change with the problem instance) and deterministic (i.e., always returns the same planning parameters). 
-    While SMAC does not exploit information specific to each problem instance, it provides an approach that has strong 
+    :class:`SMAC` is a wrapper around the `SMAC3 package <https://github.com/automl/SMAC3>`_,
+    an actively maintained tool for algorithm configuration using the latest Bayesian optimization techniques.
+    We use the `HPOFacade <https://automl.github.io/SMAC3/main/api/smac.facade.hyperparameter_optimization_facade.html#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade>`_
+    strategy provided by SMAC3.
+    In the context of the planner optimization problem, SMAC learns only a generator :math:`G_\\theta(c) = \\theta` that is unconditional
+    (i.e. does not change with the problem instance) and deterministic (i.e., always returns the same planning parameters).
+    While SMAC does not exploit information specific to each problem instance, it provides an approach that has strong
     theoretical grounding and serves as a reasonable baseline and sanity check.
-
     Note that in our implementation, each SMAC training iteration involves doing :math:`batch\\_size` planner calls on :math:`batch\\_size`
-    sampled problem instances, with the average performancec returned to SMAC. 
+    sampled problem instances, with the average performancec returned to SMAC.
     Hence, the number of training iterations should be adjusted accordingly to ensure the same amount of data (in terms of planner calls)
     is used for comparison.
     """
+
     iterations: int
     batch_size: int
 
     eval_interval: int
 
     def __init__(
         self,
@@ -89,14 +91,16 @@
         ]
         for worker in workers:
             worker.start()
 
         # Logger.
         logger: Optional[SummaryWriter] = None
         if self.eval_folder is not None:
+            if os.path.exists(self.eval_folder):
+                shutil.rmtree(self.eval_folder)
             logger = SummaryWriter(self.eval_folder)
 
         # Prepare SMAC.
         n_hyperparams = sum(
             p.trans_num_inputs for p in self.domain.composite_parameter_space()
         )
         cspace = ConfigurationSpace()
@@ -113,17 +117,18 @@
             # /tmp for now.
             output_directory=Path("/tmp"),
         )
 
         problems = self.domain.create_problem_set()
 
         iterations = -1
+        has_initial_eval = False
         p_self = self
 
-        def eval_and_log(smbo):
+        def eval_and_log(smbo, it):
             incumbent = smbo.intensifier.get_incumbent()
             if incumbent is None:
                 return
 
             params_flat = [incumbent[f"x{i}"] for i in range(n_hyperparams)]
             params = []
             extras = []
@@ -139,25 +144,33 @@
                 extras.extend(o)
                 counter += pspace.trans_num_inputs
             result = p_self.evaluator(StaticGenerator(params, extras))
             for k in result:
                 print(f" {k} = {result[k]}")
             if logger is not None:
                 for k in result.keys():
-                    logger.add_scalar(k, result[k], iterations * p_self.batch_size)
+                    logger.add_scalar(k, result[k], it)
 
         class cb(Callback):
             def on_tell_end(self, smbo, info, value):
                 nonlocal iterations
                 nonlocal p_self
+                nonlocal has_initial_eval
 
                 iterations += 1
                 print(iterations)
-                if iterations > 0 and iterations % p_self.eval_interval == 0:
-                    eval_and_log(smbo)
+
+                if (
+                    not has_initial_eval
+                    and smbo.intensifier.get_incumbent() is not None
+                ):
+                    has_initial_eval = True
+                    eval_and_log(smbo, 0)
+                elif iterations > 0 and iterations % p_self.eval_interval == 0:
+                    eval_and_log(smbo, iterations * p_self.batch_size)
 
                 if iterations >= p_self.iterations:
                     return False
 
         def obj(x: Configuration, seed: int):
 
             # Create jobs.
```

## opof/evaluators/list_evaluator.py

```diff
@@ -81,15 +81,15 @@
         generator: opof.Generator[Problem],
     ) -> Dict[str, float]:
         with torch.no_grad():
             # Precompute list in case worker times-out in between argument generation.
             for problem in self.problems:
                 (parameters, _, extras) = generator([problem])
                 parameters = [p[0].detach().cpu().numpy() for p in parameters]
-                self.job_queue.put((problem, parameters, extras))
+                self.job_queue.put((problem, parameters, [e[0] for e in extras]))
 
         # Wait for completion.
         keys = set()
         t = dict()
         c = dict()
         for _ in tqdm(range(len(self.problems)), desc="Evaluating..."):
             result = self.result_queue.get()
```

## opof/models/fc_resnet.py

```diff
@@ -17,28 +17,29 @@
 
 def create_output_layer(spaces: List[ParameterSpace]) -> nn.Module:
     return nn.LazyLinear(sum(s.dist_num_inputs for s in spaces))
 
 
 def outputs_to_parameters(
     outputs: torch.Tensor, spaces: List[ParameterSpace], samplers: nn.ModuleList
-) -> Tuple[List[torch.Tensor], Optional[torch.Tensor], List[Any]]:
+) -> Tuple[List[torch.Tensor], Optional[torch.Tensor], List[List[Any]]]:
     assert len(outputs.shape) == 2, "Invalid outputs"
     assert outputs.shape[1] == sum(s.dist_num_inputs for s in spaces), "Invalid outputs"
 
     parameters = []
     entropies: List[torch.Tensor] = []
-    extras: List[Any] = []
+    extras: List[List[Any]] = []
     offset = 0
     for (pspace, sampler) in zip(spaces, samplers):
         (p, e, o) = sampler(outputs[:, offset : offset + pspace.dist_num_inputs])
         parameters.append(p)
         if e is not None:
             entropies.append(e)
-        extras.extend(o)
+        if len(o) > 0:
+            extras.append(o)
         offset += pspace.dist_num_inputs
     return (
         parameters,
         torch.concat(entropies, dim=-1) if len(entropies) > 0 else None,
         extras,
     )
 
@@ -74,15 +75,15 @@
         super(FCResNetGenerator, self).__init__()
         self.parameter_spaces = domain.composite_parameter_space()
         self.latent_size = latent_size
         self.block_size = block_size
         self.num_blocks = num_blocks
 
         self.problem_embedding = domain.create_problem_embedding()
-        self.fc_pre = nn.Sequential(nn.LazyLinear(latent_size), nn.ReLU())
+        self.fc_pre = nn.Sequential(nn.LazyLinear(latent_size), nn.GELU())
         self.fc = nn.ModuleList(
             [
                 nn.Linear(latent_size, latent_size)
                 for _ in range(block_size * num_blocks)
             ]
         )
         if batch_norm:
@@ -97,15 +98,15 @@
         )
         self.samplers = nn.ModuleList(
             [pspace.create_sampler() for pspace in self.parameter_spaces]
         )
 
     def forward(
         self, problem: List[Problem]
-    ) -> Tuple[List[Tensor], Optional[Tensor], List[Any]]:
+    ) -> Tuple[List[Tensor], Optional[Tensor], List[List[Any]]]:
         # Embed problem.
         x = self.problem_embedding(problem)
         x = self.fc_pre(x)
 
         # Res block.
         x_skip = x
         for i in range(self.num_blocks):
@@ -159,15 +160,15 @@
         self.block_size = block_size
         self.num_blocks = num_blocks
 
         self.problem_embedding = domain.create_problem_embedding()
         self.parameters_embedding = nn.ModuleList(
             [pspace.create_embedding() for pspace in self.parameter_spaces]
         )
-        self.fc_pre = nn.Sequential(nn.LazyLinear(latent_size), nn.ReLU())
+        self.fc_pre = nn.Sequential(nn.LazyLinear(latent_size), nn.GELU())
         self.fc = nn.ModuleList(
             [
                 nn.Linear(latent_size, latent_size)
                 for _ in range(block_size * num_blocks)
             ]
         )
         if batch_norm:
```

## opof/models/static.py

```diff
@@ -5,16 +5,16 @@
 from ..generator import Generator
 
 Problem = TypeVar("Problem")
 
 
 class StaticGenerator(Generic[Problem], Generator[Problem]):
     """
-    :class:`StaticGenerator` represents a generator :math:`G_\\theta(c)` that returns 
-    fixed parameters :math:`x \\in \\mathcal{X}` and extra objects for any 
+    :class:`StaticGenerator` represents a generator :math:`G_\\theta(c)` that returns
+    fixed parameters :math:`x \\in \\mathcal{X}` and extra objects for any
     given problem instance.
     """
 
     parameters: List[Tensor]
     extras: List[Any]
 
     def __init__(self, parameters: List[Tensor], extras: List[Any]):
@@ -26,12 +26,16 @@
         """
         super(StaticGenerator, self).__init__()
         self.parameters = parameters
         self.extras = extras
 
     def forward(
         self, problem: List[Problem]
-    ) -> Tuple[List[Tensor], Optional[Tensor], List[Any]]:
-        p = []
-        for l in self.parameters:
-            p.append(l.repeat(len(problem), *[1 for _ in l.shape]))
-        return (p, None, self.extras)
+    ) -> Tuple[List[Tensor], Optional[Tensor], List[List[Any]]]:
+        parameters = []
+        extras = []
+        for p in self.parameters:
+            parameters.append(p.repeat(len(problem), *[1 for _ in p.shape]))
+        if len(self.extras) > 0:
+            for e in self.extras:
+                extras.append([e for _ in problem])
+        return (parameters, None, extras)
```

## opof/parameter_spaces/sphere.py

```diff
@@ -117,14 +117,15 @@
 
         :param inputs: Input points
         :return: Parameter values and extra objects
         """
         assert len(inputs.shape) == 2
         assert inputs.shape[1] == self.trans_num_inputs
         y = inputs.reshape(-1, self.count, self.dimension)
+        y = EPS + (1 - 2 * EPS) * y
         y = torch.special.ndtri(y)
 
         # When n is zero, the resultant vector is undefined.
         # We perturb the zero-ish elements until the norm is sufficient.
         n = np.linalg.norm(y, axis=-1, keepdims=True)
         while (n < 1e-8).any():
             y += (n < 1e-8) * (1e-4) * np.random.randn(*y.shape)
```

## Comparing `opof-0.2.0.data/scripts/opof-registry` & `opof-0.3.0.data/scripts/opof-registry`

 * *Files identical despite different names*

## Comparing `opof-0.2.0.dist-info/LICENSE.md` & `opof-0.3.0.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `opof-0.2.0.dist-info/METADATA` & `opof-0.3.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: opof
-Version: 0.2.0
+Version: 0.3.0
 Summary: Open-source framework for solving the Planner Optimization Problem
 Home-page: https://opof.kavrakilab.org
 Author: Yiyuan Lee
 Author-email: yiyuan.lee@rice.edu
 License: BSD-3
 Project-URL: Documentation, https://opof.kavrakilab.org
 Project-URL: Source, https://github.com/opoframework/opof
@@ -18,23 +18,23 @@
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE.md
 Requires-Dist: torch
 Requires-Dist: tqdm
 Requires-Dist: pyyaml
 Requires-Dist: swig
 Requires-Dist: smac (==2.0.0)
+Requires-Dist: pypop7
 Requires-Dist: scikit-learn (==1.2.0)
 Requires-Dist: tensorboard
 Provides-Extra: tests
 Requires-Dist: pytest ; extra == 'tests'
 Requires-Dist: pytest-cov ; extra == 'tests'
 
 <p align="center">
@@ -43,15 +43,15 @@
 
 OPOF, the Open-Source Planner Optimization Framework, is an open source framework for developing domains and algorithms for planner optimization. It provides a standard API to communicate between optimization algorithms and domains, along with a set of stable algorithm implementations. 
 
 Our complete documentation is available at [https://opof.kavrakilab.org](https://opof.kavrakilab.org).
 
 [![Build and Test](https://github.com/opoframework/opof/actions/workflows/build_and_test.yml/badge.svg)](https://github.com/opoframework/opof/actions/workflows/build_and_test.yml)
 
-OPOF is maintained by the [Kavraki Lab](https://kavrakilab.org) at Rice University.
+OPOF is developed and maintained by the [Kavraki Lab](https://kavrakilab.org) at Rice University.
 
 ## Algorithms
 
 OPOF includes the following stable algorithm implementations. 
 
 * [Generator-Critic (GC)](https://opof.kavrakilab.org/algorithms/GC.html) - Learns a conditional and stochastic generator using gradient-based deep learning techniques.
 * [SMAC](https://opof.kavrakilab.org/algorithms/SMAC.html) - Learns an unconditional and deterministic generator using the latest Bayesian optimization techniques. Wrapper around the actively maintained [SMAC3](https://github.com/automl/SMAC3) Bayesian optimization library. 
@@ -84,38 +84,21 @@
 
 domain = RandomWalk2D(11)
 problems = domain.create_problem_set()
 planner = domain.create_planner()
 
 parameters = [pspace.rand(100).numpy() for pspace in domain.composite_parameter_space()]
 for i in range(100):
-    result = planner(problems(), [p[i] for p in parameters])
+    result = planner(problems(), [p[i] for p in parameters], [])
     print(result["objective"])
 ```
 
-Using our built-in `opof.algorithms.GC` planner optimization algorithm is surprisingly easy.
-
-```python
-   from opof_grid2d.domains import RandomWalk2D
-   from opof.algorithms import GC
-
-   domain = RandomWalk2D(11)
-   algo = GC(domain, iterations=100000, eval_folder="results/RandomWalk2D[11]")
-   algo()
-```
-
-Evaluation results stored at ``results/RandomWalk2D[11]`` can be viewed by running
-
-```console
-  $ tensorboard --logdir=results/
-```
-
 Our complete documentation is available at [https://opof.kavrakilab.org](https://opof.kavrakilab.org).
 
 ## Citing
 TBC
 
 ## License
 
 OPOF is licensed under the [BSD-3 license](https://github.com/opoframework/opof/blob/master/LICENSE.md).
 
-OPOF is maintained by the [Kavraki Lab](https://www.kavrakilab.org/) at Rice University, funded in part by NSF RI 2008720 and Rice University funds.
+OPOF is developed and maintained by the [Kavraki Lab](https://www.kavrakilab.org/) at Rice University, funded in part by NSF RI 2008720 and Rice University funds.
```

## Comparing `opof-0.2.0.dist-info/RECORD` & `opof-0.3.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,33 +1,34 @@
 opof/__init__.py,sha256=3KQCqWtu20D4NKhDWbEm6PmW_7CfQGiMQ1ZQiSkeSHA,369
 opof/algorithm.py,sha256=LOa-T5Y1Q6_zPUfK2FOJdhAy-T4NIgS-pZX3u7ouGeM,1264
 opof/domain.py,sha256=vzGeZS9wTVBnjjj2NXRLO46ZYqRfvMlPuIOf1Yr8Gjk,2379
 opof/evaluator.py,sha256=-m0UrxO2kL_9lwjFTb9dzFhlsNC6nZJS7NLH-ZVE7K8,829
-opof/generator.py,sha256=QcoRkXjLAneyMv2LhuNPo-XqVw8eCkwMGJlBoryfBVw,1022
+opof/generator.py,sha256=CgOey9oS4W4QHiqKfus0dyE5bZ96bRpevZf6U0EFRAA,994
 opof/parameter_space.py,sha256=qXOSf95Bw0A45GrHh4e5OcSPfNCazC7rFKb-PqFnV-M,2315
-opof/planner.py,sha256=y-etqBREW_ClbaDnduUxxCjme-sbJNHSap9nV9nr97Y,1098
+opof/planner.py,sha256=BEGaWqPnzIWc0vdwWg67VYgBvahlw0FkHFU4-Uok4c8,1094
 opof/problem_set.py,sha256=NHkwxQvnFLZeyQ9rELtz0tePwliyMFv2HsW5KYlpS8w,870
 opof/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-opof/algorithms/__init__.py,sha256=nHGQOrn_4iSHPL_nTQodtDskDfzrnjAbnO_tiAspUws,68
-opof/algorithms/gc.py,sha256=Gt_IdMkVaFMJTCRtspRoTJHZQKypi7lwfd_0Y79yN6Y,15131
-opof/algorithms/smac.py,sha256=mxctsZvtH-gj_Zn5Jm-kZFxtbxlP4SvUSlEMf7F1zG0,7576
+opof/algorithms/__init__.py,sha256=mG_zjNnwNuf6-I2HMGne_JCCtYonWsZAfmTJO7Buxys,102
+opof/algorithms/gc.py,sha256=-lgAWn-Urd1Yn90oRy0DKJ663rOOtxgA0ZX_vMFitcg,16384
+opof/algorithms/pypop.py,sha256=4zbKCSW56S2Sl1UKPdp8fxWnK7VBeSPhQOupwZEe4cw,7069
+opof/algorithms/smac.py,sha256=1quE_Jck1yfYcjTXxW40fZ_Kjzj6VsfRDg86ciNEslQ,8007
 opof/evaluators/__init__.py,sha256=e1d9wtCCyek7LtBfQAXo7F74aaO6GTodhEXWGTAAqp0,71
-opof/evaluators/list_evaluator.py,sha256=8ibC7aCjv4OAjckX55SuMfL5w89nyzUcDYYnAdnq7lk,3613
+opof/evaluators/list_evaluator.py,sha256=B46vbPNExbLLFgLESoqfS8j8dulGeXdViZ1WWwvBTI8,3629
 opof/models/__init__.py,sha256=DhWmwaHPQSsucA6-ftIbhIn4s3bv0n04SE38GaoMeGo,163
-opof/models/fc_resnet.py,sha256=OEAuA3Hn_q8h0vAMIwdet4PguLJ7lsKg6UOoG891QW8,7430
-opof/models/static.py,sha256=ZTlaOsus4_4GKLOJ-DZWAylQ3CIgRH_zdo37FORwkxE,1130
+opof/models/fc_resnet.py,sha256=RyKqHqYx0YM_4yLNc7BnxJb83NQ82deK0fBpf7oWGe0,7475
+opof/models/static.py,sha256=e87q2YcA25_xmOCIuJt3hbv0SeoA8HtzttBTzSBXOIs,1295
 opof/parameter_spaces/__init__.py,sha256=vz-LUj0FfjIy86QgqYFeAl6hUKf3tz110EEcYBGRx9A,147
 opof/parameter_spaces/interval.py,sha256=Wv7n2Sou92vR0-yEqMDiaNltG4_ZqC4HpW2l1UriBjU,4578
 opof/parameter_spaces/power_spherical.py,sha256=qktlq7gStS5Z8yFRbAKcf9rZuGr3RBHMSmYq07w5Eck,7885
 opof/parameter_spaces/simplex.py,sha256=wvhlTWiTHa0_2OgtV70LkSTN7_dFNSRq8RYzqWOpnJs,5412
-opof/parameter_spaces/sphere.py,sha256=dRmTjIqrfgZflNVSx-43RCJC73FgNbc8IKUI8uYZnk0,6200
+opof/parameter_spaces/sphere.py,sha256=BBMFW0N4emqebCelvNuj9f7qlRax14QHGoS__kGrpes,6236
 opof/problem_sets/__init__.py,sha256=0MI23oDyopb0eG19uCPaPBmNn5PvQFV2Kk2d_cMyH9g,65
 opof/problem_sets/problem_list.py,sha256=7ahXWhfr2WrxYSf7RF9bVMi-KXNwEDhODvxo2BNZTdg,1495
 opof/registry/__init__.py,sha256=qQqPUM5UtIfV3061b3xM_r9cqdUNe3inpBpOrjPNHN8,388
 opof/registry/utils.py,sha256=y98n791y8mjV-osNchFXnXARSX12jOC2Gu4-Nsi6ptw,4226
-opof-0.2.0.data/scripts/opof-registry,sha256=GgNvSrrdnEInBtjI0UYCwSx3v73iVg2wZEOrB83dAj8,910
-opof-0.2.0.data/scripts/opof-train,sha256=L3YQkLH3LrTTFOmg4CM8yCE7baJcuVLDK_DRxV6-els,1711
-opof-0.2.0.dist-info/LICENSE.md,sha256=KRPxjKzLsUlcovmFwcm7ywDlLoX5d_SjNcrfJKV1m1k,1461
-opof-0.2.0.dist-info/METADATA,sha256=klc25GWp5fhJxGE23Rz7mmkwTK6JLkYy-BNkJxHHFF8,5350
-opof-0.2.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-opof-0.2.0.dist-info/top_level.txt,sha256=B-WMDRNsIerz6Am9QJpCm_NrUWEuMcuP8E-hDfK4hGA,5
-opof-0.2.0.dist-info/RECORD,,
+opof-0.3.0.data/scripts/opof-registry,sha256=GgNvSrrdnEInBtjI0UYCwSx3v73iVg2wZEOrB83dAj8,910
+opof-0.3.0.data/scripts/opof-train,sha256=m-atJWQeIfcXcxFXyokCiVLwMTraoR1hSCuh0BZ9veY,2469
+opof-0.3.0.dist-info/LICENSE.md,sha256=KRPxjKzLsUlcovmFwcm7ywDlLoX5d_SjNcrfJKV1m1k,1461
+opof-0.3.0.dist-info/METADATA,sha256=GPYVsNONKLjk7bxjL-Jpkd4ZuWOGFIPyGshFbvp9nao,4908
+opof-0.3.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+opof-0.3.0.dist-info/top_level.txt,sha256=B-WMDRNsIerz6Am9QJpCm_NrUWEuMcuP8E-hDfK4hGA,5
+opof-0.3.0.dist-info/RECORD,,
```

