# Comparing `tmp/gpudb-7.1.9.2-cp39-cp39-win_amd64.whl.zip` & `tmp/gpudb-7.1.9.3-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,11 +1,11 @@
-Zip file size: 469199 bytes, number of entries: 34
+Zip file size: 469258 bytes, number of entries: 34
 -rw-rw-rw-  2.0 fat     2276 b- defN 23-Apr-24 09:59 gpudb/__init__.py
--rw-rw-rw-  2.0 fat  2157957 b- defN 23-Apr-24 09:59 gpudb/gpudb.py
--rw-rw-rw-  2.0 fat   181209 b- defN 23-Apr-24 09:59 gpudb/gpudb_multihead_io.py
+-rw-rw-rw-  2.0 fat  2157957 b- defN 23-May-01 01:59 gpudb/gpudb.py
+-rw-rw-rw-  2.0 fat   181158 b- defN 23-May-01 01:59 gpudb/gpudb_multihead_io.py
 -rw-rw-rw-  2.0 fat    76983 b- defN 23-Apr-24 09:59 gpudb/gpudb_table_monitor.py
 -rw-rw-rw-  2.0 fat    82432 b- defN 23-Apr-24 10:03 gpudb/protocol.cp39-win_amd64.pyd
 -rw-rw-rw-  2.0 fat    33494 b- defN 23-Apr-24 09:59 gpudb/packages/enum34.py
 -rw-rw-rw-  2.0 fat    40175 b- defN 23-Apr-24 09:59 gpudb/packages/kinetica_tabulate.py
 -rw-rw-rw-  2.0 fat     4413 b- defN 23-Apr-24 09:59 gpudb/packages/ordereddict.py
 -rw-rw-rw-  2.0 fat    14620 b- defN 23-Apr-24 09:59 gpudb/packages/pymmh3.py
 -rw-rw-rw-  2.0 fat      543 b- defN 23-Apr-24 09:59 gpudb/packages/avro/__init__.py
@@ -24,13 +24,13 @@
 -rw-rw-rw-  2.0 fat    15686 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/datafile.py
 -rw-rw-rw-  2.0 fat    34387 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/io.py
 -rw-rw-rw-  2.0 fat    23138 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/ipc.py
 -rw-rw-rw-  2.0 fat    12321 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/protocol.py
 -rw-rw-rw-  2.0 fat    36528 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/schema.py
 -rw-rw-rw-  2.0 fat     5773 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/tool.py
 -rw-rw-rw-  2.0 fat     8269 b- defN 23-Apr-24 09:59 gpudb/packages/avro/avro_py3/txipc.py
--rw-rw-rw-  2.0 fat     1104 b- defN 23-Apr-24 10:03 gpudb-7.1.9.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat      371 b- defN 23-Apr-24 10:03 gpudb-7.1.9.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-24 10:03 gpudb-7.1.9.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Apr-24 10:03 gpudb-7.1.9.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3053 b- defN 23-Apr-24 10:03 gpudb-7.1.9.2.dist-info/RECORD
-34 files, 2849762 bytes uncompressed, 464269 bytes compressed:  83.7%
+-rw-rw-rw-  2.0 fat     1104 b- defN 23-May-01 02:01 gpudb-7.1.9.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat      371 b- defN 23-May-01 02:01 gpudb-7.1.9.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-01 02:01 gpudb-7.1.9.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 23-May-01 02:01 gpudb-7.1.9.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3053 b- defN 23-May-01 02:01 gpudb-7.1.9.3.dist-info/RECORD
+34 files, 2849711 bytes uncompressed, 464328 bytes compressed:  83.7%
```

## zipnote {}

```diff
@@ -81,23 +81,23 @@
 
 Filename: gpudb/packages/avro/avro_py3/tool.py
 Comment: 
 
 Filename: gpudb/packages/avro/avro_py3/txipc.py
 Comment: 
 
-Filename: gpudb-7.1.9.2.dist-info/LICENSE
+Filename: gpudb-7.1.9.3.dist-info/LICENSE
 Comment: 
 
-Filename: gpudb-7.1.9.2.dist-info/METADATA
+Filename: gpudb-7.1.9.3.dist-info/METADATA
 Comment: 
 
-Filename: gpudb-7.1.9.2.dist-info/WHEEL
+Filename: gpudb-7.1.9.3.dist-info/WHEEL
 Comment: 
 
-Filename: gpudb-7.1.9.2.dist-info/top_level.txt
+Filename: gpudb-7.1.9.3.dist-info/top_level.txt
 Comment: 
 
-Filename: gpudb-7.1.9.2.dist-info/RECORD
+Filename: gpudb-7.1.9.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gpudb/gpudb.py

```diff
@@ -4741,15 +4741,15 @@
 
     END_OF_SET = -9999
     """(int) Used for indicating that all of the records (till the end of the
     set are desired)--generally used for /get/records/\* functions.
     """
 
     # The version of this API
-    api_version = "7.1.9.2"
+    api_version = "7.1.9.3"
 
     # -------------------------  GPUdb Methods --------------------------------
 
     def __init__( self, host = None, options = None, *args, **kwargs ):
         """
         Construct a new GPUdb client instance.  This object communicates to
         the database server at the given address.  This class implements
```

## gpudb/gpudb_multihead_io.py

```diff
@@ -2472,14 +2472,17 @@
                                        datefmt = GPUdb._LOG_DATETIME_FORMAT )
         handler.setFormatter( formatter )
         self.log.addHandler( handler )
 
         # Prevent logging statements from being duplicated
         self.log.propagate = False
 
+        if (gpudb.logging_level):
+            self.log.setLevel(gpudb.logging_level)
+
         # Save the parameter values
         self.gpudb                = gpudb
         self.table_name           = table_name
         self.record_type          = record_type
         self.batch_size           = batch_size
         self.options              = options
         self.is_table_replicated  = is_table_replicated
@@ -2621,14 +2624,16 @@
 
             # We did switch to a different cluster; now check the health
             # of the cluster, starting with the head node
             if not self.gpudb.is_kinetica_running( curr_url ):
                 continue # try the next cluster because this head node is down
             # end if
 
+            # Check that all the rank URLs are functional for the new cluster if
+            # using multi-head
             is_cluster_healthy = True
             if self.is_multihead_enabled:
                 # Obtain the worker rank addresses
                 try:
                     worker_ranks = GPUdbWorkerList( self.gpudb,
                                                     ip_regex = self.worker_list.get_ip_regex(),
                                                     use_head_node_only = self.use_head_node )
@@ -2667,33 +2672,34 @@
         raise GPUdbException( error_msg )
     # end __force_failover
 
 
     def __update_worker_queues( self, count_cluster_switches,
                                 do_reconstruct_worker_queues = True ):
         """Update the shard mapping for the ingestor.
+        
+        Note:  This needs to reconstruct the worker queues even in head
+               node-only mode, as the flush call will use the
+               worker URL regardless of what mode it's in.  In head node-only
+               mode, there will be one "worker" with the rank 0 URL.
 
         Parameters:
             count_cluster_switches (int)
                 Integer keeping track of how many times inter-cluster failover
                 has happened.
             do_reconstruct_worker_queues (bool)
                 When True, the worker queues will be re-constructed based on
                 the new cluster configuration.  The records that are already in
                 the existing queues will be re-processed to be saved in the
                 new queues.
 
         Returns:
             A boolean flag indicating if the shard mapping was updated.
         """
-        # Decide if the worker queues will need to be reconstructed (they will
-        # only if multi-head is enabled, it is not a replicated table, and if
-        # the user wants to)
-        reconstruct_worker_queues = ( do_reconstruct_worker_queues
-                                      and (not self.use_head_node) )
+
         try:
             # Get the sharding assignment ranks
             shard_info = self.gpudb.admin_show_shards()
 
             if not shard_info.is_ok():
                 raise GPUdbException( shard_info.get_error_msg() )
 
@@ -2705,15 +2711,15 @@
                 # Also check if the db client has failed over to a different HA
                 # ring node
                 num_db_ha_switches = self.gpudb.get_num_cluster_switches()
                 if (count_cluster_switches == num_db_ha_switches):
                     self.__log_debug( "# cluster switches and shard versions "
                                       "the same" )
 
-                    if reconstruct_worker_queues:
+                    if do_reconstruct_worker_queues:
                         # The caller needs to know if we ended up updating the
                         # queues
                         return self.__reconstruct_worker_queues_and_requeue_records()
                     # end if
 
                     # Not appropriate to update worker queues; then no change
                     # has happened
@@ -2756,17 +2762,15 @@
 
         # If we get here, then we may have done a cluster failover during
         # /admin/show/shards; so update the current head node url & count of
         # cluster switches
         self.__curr_head_node_url = self.gpudb.get_url( stringified = False )
         self.num_cluster_switches      = self.gpudb.get_num_cluster_switches()
 
-        # The worker queues need to be re-constructed when asked for
-        # iff multi-head i/o is enabled and the table is not replicated
-        if reconstruct_worker_queues:
+        if do_reconstruct_worker_queues:
             self.__reconstruct_worker_queues_and_requeue_records()
 
         self.__log_debug( "Returning true" )
         return True # the shard mapping was updated indeed
     # end __update_worker_queues
 
 
@@ -3620,14 +3624,17 @@
                                        datefmt = GPUdb._LOG_DATETIME_FORMAT )
         handler.setFormatter( formatter )
         self.log.addHandler( handler )
 
         # Prevent logging statements from being duplicated
         self.log.propagate = False
 
+        if (gpudb.logging_level):
+            self.log.setLevel(gpudb.logging_level)
+
         # Save the parameter values
         self.gpudb       = gpudb
         self.table_name  = table_name
         self.record_type = record_type
         self.worker_list = workers
         self.is_table_replicated = is_table_replicated
 
@@ -3811,14 +3818,16 @@
             if not self.gpudb.is_kinetica_running( curr_url ):
                 continue # try the next cluster because this head node is down
             # end if
 
             # Check if we switched the rank-0 URL
             did_switch_url = (curr_url != old_url)
 
+            # Check that all the rank URLs are functional for the new cluster if
+            # using multi-head
             is_cluster_healthy = True
             if self.is_multihead_enabled:
                 # Obtain the worker rank addresses
                 try:
                     worker_ranks = GPUdbWorkerList( self.gpudb,
                                                     ip_regex = self.worker_list.get_ip_regex(),
                                                     use_head_node_only = self.use_head_node )
@@ -3859,34 +3868,32 @@
 
     def __update_worker_queues( self, count_cluster_switches,
                                 do_reconstruct_worker_queues = True ):
         """Updates the shard mapping based on the latest cluster configuration.
         Optionally, also reconstructs the worker queues based on the new
         sharding.
 
+        Note:  This needs to reconstruct the worker queues even in head
+               node-only mode, as the get_records_by_key call will use the
+               worker URL regardless of what mode it's in.  In head node-only
+               mode, there will be one "worker" with the rank 0 URL.
+
         Parameters:
             count_cluster_switches (int)
                 Integer keeping track of how many times inter-cluster failover
                 has happened.
             do_reconstruct_worker_queues (bool)
                 When True, the worker queues will be re-constructed based on
                 the new cluster configuration.  The records that are already in
                 the existing queues will be re-processed to be saved in the
                 new queues.
 
         Returns:
             A boolean flag indicating if the shard mapping was updated.
         """
-        # Decide if the worker queues will need to be reconstructed (they will
-        # only if multi-head is enabled, it is not a replicated table, and if
-        # the user wants to)
-        reconstruct_worker_queues = ( do_reconstruct_worker_queues
-                                      and (not self.use_head_node) )
-        self.__log_debug( "Reconstruct worker URLs?: {}"
-                          "".format( reconstruct_worker_queues ) )
 
         try:
             # Get the sharding assignment ranks
             shard_info = self.gpudb.admin_show_shards()
 
             if not shard_info.is_ok():
                 raise GPUdbException( shard_info.get_error_msg() )
@@ -3899,15 +3906,15 @@
                 # Also check if the db client has failed over to a different HA
                 # ring node
                 num_cluster_switches = self.gpudb.get_num_cluster_switches()
                 if (count_cluster_switches == num_cluster_switches):
                     self.__log_debug( "# cluster switches and shard versions "
                                       "the same" )
 
-                    if reconstruct_worker_queues:
+                    if do_reconstruct_worker_queues:
                         # The caller needs to know if we ended up updating the
                         # queues
                         return self.__reconstruct_worker_queues()
                     # end if
 
                     # Not appropriate to update worker queues; then no change
                     # has happened
@@ -3943,17 +3950,15 @@
 
         # If we get here, then we may have done a cluster failover during
         # /admin/show/shards; so update the current head node url & count of
         # cluster switches
         self.__curr_head_node_url = self.gpudb.get_url( stringified = False )
         self.num_cluster_switches = self.gpudb.get_num_cluster_switches()
 
-        # The worker queues need to be re-constructed when asked for
-        # iff multi-head i/o is enabled and the table is not replicated
-        if reconstruct_worker_queues:
+        if do_reconstruct_worker_queues:
             self.__reconstruct_worker_queues()
 
         self.__log_debug( "Returning true" )
         return True # the shard mapping was updated indeed
     # end __update_worker_queues
```

## Comparing `gpudb-7.1.9.2.dist-info/LICENSE` & `gpudb-7.1.9.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `gpudb-7.1.9.2.dist-info/RECORD` & `gpudb-7.1.9.3.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 gpudb/__init__.py,sha256=MTfrxWZ7WJ5APTDA90SyH_MMwb9mygIcZsCkvkPZnUo,2276
-gpudb/gpudb.py,sha256=s9rmVJghODM7QqEgIg-T8SHt-8dkySA33cbUDzRPel0,2157957
-gpudb/gpudb_multihead_io.py,sha256=akuQZ65VQqMV5xyH65Z-IX5Op_VhDoon4kzTulvDp-A,181209
+gpudb/gpudb.py,sha256=dgg5r46rPX2S2hgZ6qY5ILQXhZS83lLJuWDv8G9YTXA,2157957
+gpudb/gpudb_multihead_io.py,sha256=ApIRL7ufTqNEyWKnLnsvbM-BEjmhHf11L0jTgdf82ac,181158
 gpudb/gpudb_table_monitor.py,sha256=YxzWUzp2FVhLML0XPCGRLq6Vh2a2--8oozEEjjswJ9A,76983
 gpudb/protocol.cp39-win_amd64.pyd,sha256=Cf-nuYrBG_gPViFseyxgjjzre4Nd95KmtTxQ1c4s2E4,82432
 gpudb/packages/enum34.py,sha256=M1JeCFaL3T7ZWVlu_J4OeNuil8SGJ6cNgp9JzEwn8zE,33494
 gpudb/packages/kinetica_tabulate.py,sha256=RTcY5vDKdGwc4WvUyxXLX5dWDoUt2Gt50lVqUxfPf_0,40175
 gpudb/packages/ordereddict.py,sha256=EuzwpZHmxsQnWP2Xa2U5ApZU19YOijgQJCbYx20AvJ4,4413
 gpudb/packages/pymmh3.py,sha256=PkbkLgOTeERrTQZZ9Yrr1JwJHvA9vFbYMS3Tt553Uuc,14620
 gpudb/packages/avro/__init__.py,sha256=2jFb1uc5rCpZQIT6RwnttmJIRMFTg3Pyj4IIVv04L7w,543
@@ -23,12 +23,12 @@
 gpudb/packages/avro/avro_py3/datafile.py,sha256=N5W1D8YU7YacVsEFi-1_PDTKWYWXcMa505pSfyRlNuU,15686
 gpudb/packages/avro/avro_py3/io.py,sha256=OZo_LWbeft4NGCelKx2RHukMV955LVJ7t3nVQBdf_Ws,34387
 gpudb/packages/avro/avro_py3/ipc.py,sha256=nPY5A-eo0t9hFHFGqqH4ksl0-ixFItUjE_hajJeUkKM,23138
 gpudb/packages/avro/avro_py3/protocol.py,sha256=9xGYO3_2pxZol6uTg7IVZq6YgXvfyjB6ZgCd1kzGjkY,12321
 gpudb/packages/avro/avro_py3/schema.py,sha256=rRvAXnY_y-CHOp9XPTr_1x2oDFvHW0yPWLBnVEMmHuA,36528
 gpudb/packages/avro/avro_py3/tool.py,sha256=tEHYWcbMgnficW8mIdBVR32VsFt9lV1mH37eqtMG51E,5773
 gpudb/packages/avro/avro_py3/txipc.py,sha256=S21zvYQkxUtW0AmakYIzYDQoUsL_gXnIJbph5xWdBnI,8269
-gpudb-7.1.9.2.dist-info/LICENSE,sha256=mO8T_HZHSioc8ubjlLCgRqkSEqu6jK7tXsBhPGxk6OI,1104
-gpudb-7.1.9.2.dist-info/METADATA,sha256=Tvczn-6112ehCBGZXnYwZrtot2YVPhYRCtsoD6t64q4,371
-gpudb-7.1.9.2.dist-info/WHEEL,sha256=jr7ubY0Lkz_yXH9FfFe9PTtLhGOsf62dZkNvTYrJINE,100
-gpudb-7.1.9.2.dist-info/top_level.txt,sha256=WXblkeM06dfvsdiZ156hrZR9H6m3fEQ6L0rUV7YqDAM,6
-gpudb-7.1.9.2.dist-info/RECORD,,
+gpudb-7.1.9.3.dist-info/LICENSE,sha256=mO8T_HZHSioc8ubjlLCgRqkSEqu6jK7tXsBhPGxk6OI,1104
+gpudb-7.1.9.3.dist-info/METADATA,sha256=s9Osx10-47_J1t7misGz8vq5uUqGIesPdU1v9yatpsU,371
+gpudb-7.1.9.3.dist-info/WHEEL,sha256=jr7ubY0Lkz_yXH9FfFe9PTtLhGOsf62dZkNvTYrJINE,100
+gpudb-7.1.9.3.dist-info/top_level.txt,sha256=WXblkeM06dfvsdiZ156hrZR9H6m3fEQ6L0rUV7YqDAM,6
+gpudb-7.1.9.3.dist-info/RECORD,,
```

